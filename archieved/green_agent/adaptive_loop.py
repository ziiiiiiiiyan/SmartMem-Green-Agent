"""
Adaptive Adversarial Testing Loop (è‡ªé€‚åº”å¯¹æŠ—æµ‹è¯•å¾ªç¯)

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»ç®€å•åˆ°éš¾é€æ­¥æµ‹è¯• Agent
2. åˆ†æå¤±è´¥æ¨¡å¼ï¼Œè¯†åˆ«å¼±ç‚¹
3. é’ˆå¯¹å¼±ç‚¹ç”Ÿæˆæ›´å¤šç±»ä¼¼/æ›´éš¾çš„é—®é¢˜
4. å¾ªç¯ç›´åˆ°æ‘¸æ¸…èƒ½åŠ›è¾¹ç•Œ
5. è¾“å‡ºé‡åŒ–çš„å¼±ç‚¹æŠ¥å‘Šï¼ˆå«é›·è¾¾å›¾ï¼‰

æ¶æ„è¯´æ˜:
- Agent æ˜¯é»‘ç›’ï¼Œåªé€šè¿‡ chat(instruction) -> response äº¤äº’
- æ”¯æŒä»»æ„å®ç° AgentInterface çš„ Agent
- Eval è´Ÿè´£è§£æå“åº”ã€æ‰§è¡ŒåŠ¨ä½œã€è¯„åˆ†

ä½¿ç”¨æ–¹æ³•:
    # ä½¿ç”¨ Mock Agent æµ‹è¯•
    python adaptive_loop.py --agent-type mock --error-rate 0.3
    
    # ä½¿ç”¨ OpenAI API
    python adaptive_loop.py --agent-type openai --model gpt-4o
    
    # ä½¿ç”¨ Purple Agent
    python adaptive_loop.py --agent-type purple
"""

import json
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import math

sys.path.insert(0, str(Path(__file__).parent.parent))

from green_agent import GreenAgent, TestCase, DEVICE_CONSTRAINTS, DIMENSIONS
from green_agent.agent_interface import AgentInterface, create_agent, MockAgent
from green_agent.blackbox_eval import BlackBoxEvaluator, ResponseParser
from app.environment import SmartHomeEnv
from app.evaluator import TurnEvaluator

# ============== æ•°æ®ç»“æ„ ==============

@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    test_case: dict
    score: float  # 0.0 - 1.0
    max_score: float
    passed: bool
    errors: List[str] = field(default_factory=list)
    turn_details: List[dict] = field(default_factory=list)


@dataclass 
class DimensionStats:
    """ç»´åº¦ç»Ÿè®¡"""
    total: int = 0
    passed: int = 0
    failed: int = 0
    total_score: float = 0.0
    max_possible_score: float = 0.0
    
    @property
    def pass_rate(self) -> float:
        return self.passed / max(1, self.total)
    
    @property
    def avg_score(self) -> float:
        return self.total_score / max(1, self.max_possible_score)
    
    @property
    def weakness_score(self) -> float:
        """å¼±ç‚¹åˆ†æ•°ï¼šè¶Šé«˜è¶Šå¼± (0-1)"""
        return 1.0 - self.avg_score


@dataclass
class WeaknessProfile:
    """å¼±ç‚¹ç”»åƒ"""
    # æŒ‰ç»´åº¦ç»Ÿè®¡
    by_dimension: Dict[str, DimensionStats] = field(default_factory=dict)
    # æŒ‰éš¾åº¦ç»Ÿè®¡
    by_difficulty: Dict[str, DimensionStats] = field(default_factory=dict)
    # æŒ‰è®¾å¤‡ç»Ÿè®¡
    by_device: Dict[str, DimensionStats] = field(default_factory=dict)
    # å¤±è´¥ç”¨ä¾‹åˆ—è¡¨
    failed_cases: List[TestResult] = field(default_factory=list)
    # è¾¹ç•Œå‘ç°
    boundary_found: Dict[str, str] = field(default_factory=dict)  # dimension -> difficulty


# ============== é»‘ç›’ Agent åŒ…è£…å™¨ ==============

class BlackBoxAgentWrapper:
    """
    é»‘ç›’ Agent åŒ…è£…å™¨
    
    å°†ä»»æ„ Agentï¼ˆé€šè¿‡ AgentInterfaceï¼‰åŒ…è£…æˆé»‘ç›’å½¢å¼ï¼Œ
    åªé€šè¿‡æ–‡æœ¬ I/O è¿›è¡Œäº¤äº’ï¼Œç”± Eval æ¡†æ¶è´Ÿè´£è§£æå“åº”å¹¶æ‰§è¡ŒåŠ¨ä½œã€‚
    
    è¿™å®ç°äº†:
    - eval æ¡†æ¶èµ·ç¯å¢ƒ + æä¾›å·¥å…·å‡½æ•°
    - agent åªè´Ÿè´£æ–‡æœ¬è¾“å‡º
    - eval æ¡†æ¶è§£æ agent è¾“å‡ºå¹¶æ‰§è¡Œ
    """
    
    def __init__(
        self, 
        env: SmartHomeEnv, 
        agent: Optional[AgentInterface] = None,
        agent_type: str = "mock",
        **kwargs
    ):
        """
        åˆå§‹åŒ–é»‘ç›’ Agent åŒ…è£…å™¨
        
        Args:
            env: SmartHome ç¯å¢ƒ
            agent: å®ç° AgentInterface çš„ Agent å®ä¾‹
            agent_type: å¦‚æœæœªæä¾› agentï¼Œåˆ™ä½¿ç”¨æ­¤ç±»å‹åˆ›å»º
                - "mock": æ¨¡æ‹Ÿ Agent (æŒ‰é¢„æœŸè¾“å‡º)
                - "openai": OpenAI API Agent
                - "anthropic": Anthropic Claude Agent
                - "ollama": Ollama æœ¬åœ° Agent
                - "purple": Purple Agent åŒ…è£…å™¨
            **kwargs: Agent åˆ›å»ºå‚æ•°
        """
        self.env = env
        self.kwargs = kwargs
        self.response_parser = ResponseParser()
        
        # åˆå§‹åŒ– Agent
        if agent is not None:
            self.agent = agent
        else:
            self.agent = self._create_agent(agent_type, **kwargs)
        
        self.agent_type = agent_type
    
    def _create_agent(self, agent_type: str, **kwargs) -> AgentInterface:
        """åˆ›å»º Agent å®ä¾‹"""
        return create_agent(agent_type, **kwargs)
    
    def execute_turn(self, instruction: str, expected_actions: Optional[List[dict]] = None) -> Tuple[List[dict], dict]:
        """
        æ‰§è¡Œä¸€è½®å¯¹è¯ï¼ˆé»‘ç›’æ¨¡å¼ï¼‰
        
        æµç¨‹:
        1. å‘é€æŒ‡ä»¤ç»™ Agent
        2. è·å– Agent çš„æ–‡æœ¬å“åº”
        3. è§£æå“åº”ä¸­çš„åŠ¨ä½œ
        4. åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
        5. è¿”å›æ‰§è¡Œç»“æœ
        
        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
            expected_actions: é¢„æœŸåŠ¨ä½œï¼ˆä»…ç”¨äº mock æ¨¡å¼ï¼‰
        
        Returns:
            (actual_actions, final_state)
        """
        # é‡ç½®ç¯å¢ƒçš„ action history
        self.env.reset_turn_history()
        
        # æ„å»ºå¸¦ç¯å¢ƒçŠ¶æ€çš„æç¤º
        current_state = self.env.get_state()['state']
        prompt = self._build_prompt(instruction, current_state)
        
        # è·å– Agent å“åº”
        if isinstance(self.agent, MockAgent):
            # Mock æ¨¡å¼ï¼šç›´æ¥è¿”å›é¢„æœŸåŠ¨ä½œ
            response = self._mock_response(expected_actions or [])
        elif isinstance(self.agent, ImperfectMockAgent):
            # Imperfect Mock æ¨¡å¼ï¼šè®¾ç½®é¢„æœŸåŠ¨ä½œåè¿”å›å¸¦é”™è¯¯çš„å“åº”
            self.agent.set_expected_actions(expected_actions or [])
            response = self.agent.chat(prompt)
        else:
            # çœŸå® Agentï¼šè·å–æ–‡æœ¬å“åº”
            response = self.agent.chat(prompt)
        
        # è§£æå“åº”ä¸­çš„åŠ¨ä½œ
        parsed_actions = self.response_parser.parse(response)
        
        # åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
        actual_actions = []
        for action in parsed_actions:
            if action.get('action') == 'update':
                key = action.get('key')
                value = action.get('value')
                
                result = self.env.update_state(key, value)
                if result['status'] == 'success':
                    actual_actions.append(action)
                    self.env.record_action(action)
        
        final_state = self.env.get_state()['state']
        return actual_actions, final_state
    
    def _build_prompt(self, instruction: str, current_state: dict) -> str:
        """æ„å»ºå‘é€ç»™ Agent çš„å®Œæ•´æç¤º"""
        state_str = json.dumps(current_state, ensure_ascii=False, indent=2)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¶å±…æ§åˆ¶åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æŒ‡ä»¤æ§åˆ¶è®¾å¤‡ã€‚

å½“å‰è®¾å¤‡çŠ¶æ€:
```json
{state_str}
```

ç”¨æˆ·æŒ‡ä»¤: {instruction}

è¯·è¾“å‡ºä½ è¦æ‰§è¡Œçš„åŠ¨ä½œã€‚ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼:
```json
{{"actions": [{{"action": "update", "key": "è®¾å¤‡å", "value": "æ–°çŠ¶æ€"}}]}}
```

å¦‚æœéœ€è¦æ‰§è¡Œå¤šä¸ªåŠ¨ä½œï¼Œåœ¨ actions æ•°ç»„ä¸­æ·»åŠ å¤šä¸ªå¯¹è±¡ã€‚
å¦‚æœä¸éœ€è¦æ‰§è¡Œä»»ä½•åŠ¨ä½œï¼Œè¿”å›ç©ºæ•°ç»„: {{"actions": []}}
"""
        return prompt
    
    def _mock_response(self, expected_actions: List[dict]) -> str:
        """ç”Ÿæˆ Mock å“åº”"""
        return json.dumps({"actions": expected_actions}, ensure_ascii=False)
    
    def reset(self, initial_state: Optional[dict] = None):
        """é‡ç½®ç¯å¢ƒå’Œ Agent"""
        self.env.reset(initial_state=initial_state)
        self.agent.reset()
    
    def get_agent_info(self) -> dict:
        """è·å– Agent ä¿¡æ¯"""
        return {
            "type": self.agent_type,
            "interface": type(self.agent).__name__,
            "kwargs": {k: v for k, v in self.kwargs.items() if k != 'api_key'}
        }


class ImperfectMockAgent(AgentInterface):
    """å¸¦éšæœºé”™è¯¯çš„ Mock Agentï¼ˆç”¨äºæµ‹è¯•æ¡†æ¶æœ¬èº«ï¼‰"""
    
    def __init__(self, error_rate: float = 0.2):
        self.error_rate = error_rate
        self.expected_actions = []
    
    def set_expected_actions(self, actions: List[dict]):
        """è®¾ç½®é¢„æœŸåŠ¨ä½œ"""
        self.expected_actions = actions
    
    def chat(self, message: str) -> str:
        """è¿”å›å¸¦éšæœºé”™è¯¯çš„å“åº”"""
        import random
        
        # è¿‡æ»¤æ‰ä¸€äº›åŠ¨ä½œï¼ˆæ¨¡æ‹Ÿé—æ¼ï¼‰
        filtered_actions = []
        for action in self.expected_actions:
            if random.random() >= self.error_rate:
                filtered_actions.append(action)
        
        return json.dumps({"actions": filtered_actions}, ensure_ascii=False)
    
    def reset(self):
        self.expected_actions = []
    
    def get_tool_calls(self, response: str = "") -> List[dict]:
        return self.expected_actions


# ============== å‘åå…¼å®¹çš„ BaselineAgent ==============

class BaselineAgent(BlackBoxAgentWrapper):
    """
    å‘åå…¼å®¹çš„ BaselineAgent ç±»
    
    è¿™æ˜¯ BlackBoxAgentWrapper çš„åˆ«åï¼Œä¿æŒ API å…¼å®¹æ€§ã€‚
    æ–°ä»£ç å»ºè®®ç›´æ¥ä½¿ç”¨ BlackBoxAgentWrapperã€‚
    """
    
    def __init__(self, env: SmartHomeEnv, agent_type: str = "mock", **kwargs):
        """
        åˆå§‹åŒ– Baseline Agent
        
        Args:
            env: SmartHome ç¯å¢ƒ
            agent_type: Agent ç±»å‹
                - "mock" / "simulated": æ¨¡æ‹Ÿ Agent (æŒ‰é¢„æœŸåŠ¨ä½œæ‰§è¡Œ)
                - "imperfect": å¸¦éšæœºé”™è¯¯çš„æ¨¡æ‹Ÿ Agent
                - "purple_agent" / "purple": Purple Agent
                - "openai": OpenAI API Agent
                - "anthropic": Anthropic Claude Agent
                - "ollama": Ollama æœ¬åœ° Agent
            **kwargs: å…¶ä»–å‚æ•°
        """
        # æ˜ å°„æ—§çš„ç±»å‹åç§°
        type_mapping = {
            "simulated": "mock",
            "purple_agent": "purple"
        }
        mapped_type = type_mapping.get(agent_type, agent_type)
        
        # imperfect æ¨¡å¼ä½¿ç”¨ç‰¹æ®Šçš„ Mock Agent
        if agent_type == "imperfect":
            error_rate = kwargs.get('error_rate', 0.2)
            agent = ImperfectMockAgent(error_rate=error_rate)
            super().__init__(env, agent=agent, agent_type="imperfect", **kwargs)
        else:
            super().__init__(env, agent_type=mapped_type, **kwargs)


# ============== è¯„ä¼°å™¨ ==============

class AdaptiveEvaluator:
    """è‡ªé€‚åº”è¯„ä¼°å™¨"""
    
    def __init__(self, agent: BaselineAgent):
        self.agent = agent
        self.env = agent.env
    
    def evaluate_test_case(self, test_case: dict) -> TestResult:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        
        # é‡ç½®ç¯å¢ƒ
        initial_state = test_case.get('initial_state', {})
        self.agent.reset(initial_state)
        
        total_score = 0.0
        max_score = 0.0
        turn_details = []
        all_errors = []
        
        for turn in test_case.get('turns', []):
            turn_id = turn.get('turn_id', 0)
            instruction = turn.get('gm_instruction', '')
            expected_actions = turn.get('expected_agent_action', [])
            expected_state = turn.get('expected_final_state', {})
            
            # é‡ç½® turn å†å²
            self.env.reset_turn_history()
            
            # Agent æ‰§è¡Œ
            actual_actions, actual_state = self.agent.execute_turn(instruction, expected_actions)
            
            # è¯„åˆ†
            evaluator = TurnEvaluator(expected_actions, expected_state)
            result = evaluator.evaluate(actual_actions, actual_state)
            
            turn_score = result['score']
            turn_max = 1.0
            
            total_score += turn_score
            max_score += turn_max
            
            turn_details.append({
                'turn_id': turn_id,
                'instruction': instruction,
                'score': turn_score,
                'max_score': turn_max,
                'passed': turn_score == turn_max,
                'errors': result.get('details', {}).get('errors', [])
            })
            
            if result.get('details', {}).get('errors'):
                all_errors.extend(result['details']['errors'])
        
        # è®¡ç®—æ€»åˆ†
        final_score = total_score / max(1, max_score)
        passed = final_score >= 1.0
        
        return TestResult(
            test_case=test_case,
            score=total_score,
            max_score=max_score,
            passed=passed,
            errors=all_errors,
            turn_details=turn_details
        )
    
    def evaluate_batch(self, test_cases: List[dict]) -> List[TestResult]:
        """æ‰¹é‡è¯„ä¼°"""
        results = []
        for case in test_cases:
            result = self.evaluate_test_case(case)
            results.append(result)
        return results


# ============== å¼±ç‚¹åˆ†æå™¨ ==============

class WeaknessAnalyzer:
    """å¼±ç‚¹åˆ†æå™¨"""
    
    def __init__(self):
        self.profile = WeaknessProfile()
        # åˆå§‹åŒ–å„ç»´åº¦ç»Ÿè®¡
        for dim in DIMENSIONS:
            self.profile.by_dimension[dim] = DimensionStats()
        for diff in ['easy', 'medium', 'difficult']:
            self.profile.by_difficulty[diff] = DimensionStats()
        for device in DEVICE_CONSTRAINTS.keys():
            self.profile.by_device[device] = DimensionStats()
    
    def analyze(self, results: List[TestResult]) -> WeaknessProfile:
        """åˆ†ææµ‹è¯•ç»“æœï¼Œæ›´æ–°å¼±ç‚¹ç”»åƒ"""
        
        for result in results:
            case = result.test_case
            dimension = case.get('dimension', 'unknown')
            difficulty = case.get('difficulty', 'unknown')
            
            # æ›´æ–°ç»´åº¦ç»Ÿè®¡
            if dimension in self.profile.by_dimension:
                self._update_stats(self.profile.by_dimension[dimension], result)
            
            # æ›´æ–°éš¾åº¦ç»Ÿè®¡
            if difficulty in self.profile.by_difficulty:
                self._update_stats(self.profile.by_difficulty[difficulty], result)
            
            # æ›´æ–°è®¾å¤‡ç»Ÿè®¡
            devices_involved = self._extract_devices(case)
            for device in devices_involved:
                if device in self.profile.by_device:
                    self._update_stats(self.profile.by_device[device], result)
            
            # è®°å½•å¤±è´¥ç”¨ä¾‹
            if not result.passed:
                self.profile.failed_cases.append(result)
        
        # æ£€æµ‹èƒ½åŠ›è¾¹ç•Œ
        self._detect_boundaries()
        
        return self.profile
    
    def _update_stats(self, stats: DimensionStats, result: TestResult):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        stats.total += 1
        stats.total_score += result.score
        stats.max_possible_score += result.max_score
        if result.passed:
            stats.passed += 1
        else:
            stats.failed += 1
    
    def _extract_devices(self, case: dict) -> set:
        """æå–æ¶‰åŠçš„è®¾å¤‡"""
        devices = set()
        
        # ä» initial_state
        for key in case.get('initial_state', {}).keys():
            devices.add(key)
        
        # ä» turns
        for turn in case.get('turns', []):
            for action in turn.get('expected_agent_action', []):
                if 'key' in action:
                    devices.add(action['key'])
            for key in turn.get('expected_final_state', {}).keys():
                devices.add(key)
        
        return devices
    
    def _detect_boundaries(self):
        """æ£€æµ‹èƒ½åŠ›è¾¹ç•Œ"""
        
        # å¯¹æ¯ä¸ªç»´åº¦ï¼Œæ‰¾åˆ°å¼€å§‹å¤±è´¥çš„éš¾åº¦
        for dim in DIMENSIONS:
            dim_stats = self.profile.by_dimension.get(dim, DimensionStats())
            
            if dim_stats.total == 0:
                continue
            
            # ç®€å•åˆ¤æ–­ï¼šå¦‚æœé€šè¿‡ç‡ä½äº 50%ï¼Œè®¤ä¸ºè¾¾åˆ°è¾¹ç•Œ
            if dim_stats.pass_rate < 0.5:
                # å°è¯•æ‰¾åˆ°å…·ä½“æ˜¯å“ªä¸ªéš¾åº¦å¼€å§‹å¤±è´¥
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´ç»†è‡´çš„åˆ†æ
                if self.profile.by_difficulty['easy'].pass_rate < 0.5:
                    self.profile.boundary_found[dim] = 'easy'
                elif self.profile.by_difficulty['medium'].pass_rate < 0.5:
                    self.profile.boundary_found[dim] = 'medium'
                else:
                    self.profile.boundary_found[dim] = 'difficult'
    
    def get_top_weaknesses(self, n: int = 5) -> List[Tuple[str, str, float]]:
        """è·å–æœ€å¼±çš„ N ä¸ªç»´åº¦/è®¾å¤‡ç»„åˆ"""
        weaknesses = []
        
        # ç»´åº¦å¼±ç‚¹
        for dim, stats in self.profile.by_dimension.items():
            if stats.total > 0:
                weaknesses.append(('dimension', dim, stats.weakness_score))
        
        # è®¾å¤‡å¼±ç‚¹
        for device, stats in self.profile.by_device.items():
            if stats.total > 0:
                weaknesses.append(('device', device, stats.weakness_score))
        
        # æŒ‰å¼±ç‚¹åˆ†æ•°æ’åº
        weaknesses.sort(key=lambda x: x[2], reverse=True)
        return weaknesses[:n]


# ============== è‡ªé€‚åº”ç”Ÿæˆç­–ç•¥ ==============

class AdaptiveGenerator:
    """è‡ªé€‚åº”ç”Ÿæˆç­–ç•¥"""
    
    def __init__(self, green_agent: GreenAgent):
        self.green_agent = green_agent
    
    def generate_targeted(
        self, 
        weaknesses: List[Tuple[str, str, float]], 
        count_per_weakness: int = 5,
        difficulty_boost: bool = True
    ) -> List[TestCase]:
        """é’ˆå¯¹å¼±ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        
        generated = []
        
        for weakness_type, weakness_name, weakness_score in weaknesses:
            # ç¡®å®šç”Ÿæˆå‚æ•°
            if weakness_type == 'dimension':
                dimension = weakness_name
                # æ ¹æ®å¼±ç‚¹åˆ†æ•°å†³å®šéš¾åº¦
                if weakness_score > 0.7:
                    difficulty = 'easy'  # å¼±ç‚¹å¾ˆæ˜æ˜¾ï¼Œç”¨ç®€å•é¢˜ç¡®è®¤
                elif weakness_score > 0.4:
                    difficulty = 'medium'
                else:
                    difficulty = 'difficult'  # å¼±ç‚¹ä¸æ˜æ˜¾ï¼Œç”¨éš¾é¢˜æ¢æµ‹
            else:
                # è®¾å¤‡å¼±ç‚¹ï¼Œç”¨ precision ç»´åº¦æµ‹è¯•
                dimension = 'precision'
                difficulty = 'medium'
            
            if difficulty_boost:
                # é€æ­¥æå‡éš¾åº¦
                difficulties = ['easy', 'medium', 'difficult']
                current_idx = difficulties.index(difficulty)
                if current_idx < len(difficulties) - 1:
                    difficulty = difficulties[current_idx + 1]
            
            print(f"  ğŸ¯ é’ˆå¯¹å¼±ç‚¹ [{weakness_type}: {weakness_name}] ç”Ÿæˆ {count_per_weakness} ä¸ª {difficulty} ç”¨ä¾‹")
            
            for i in range(count_per_weakness):
                case = self.green_agent.generate_single_case(
                    difficulty=difficulty,
                    dimension=dimension,
                    scenario_number=len(generated) + 1
                )
                if case:
                    generated.append(case)
        
        return generated


# ============== æŠ¥å‘Šç”Ÿæˆå™¨ ==============

class ReportGenerator:
    """å¼±ç‚¹æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.last_report_path: Path | None = None
        self.last_data_path: Path | None = None
    
    def generate_report(
        self, 
        profile: WeaknessProfile, 
        round_history: List[dict],
        agent_name: str = "Purple Agent"
    ) -> str:
        """ç”Ÿæˆå®Œæ•´çš„å¼±ç‚¹æŠ¥å‘Š"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"weakness_report_{timestamp}.md"
        
        # è®¡ç®—é›·è¾¾å›¾æ•°æ®
        radar_data = self._compute_radar_data(profile)
        
        report = []
        report.append(f"# ğŸ¯ Agent èƒ½åŠ›è¯„ä¼°æŠ¥å‘Š")
        report.append(f"\n**è¯„ä¼°å¯¹è±¡**: {agent_name}")
        report.append(f"**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**æµ‹è¯•è½®æ•°**: {len(round_history)}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_cases = sum(r.get('total_cases', 0) for r in round_history)
        total_passed = sum(r.get('passed', 0) for r in round_history)
        report.append(f"\n## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        report.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
        report.append(f"|------|------|")
        report.append(f"| æ€»æµ‹è¯•ç”¨ä¾‹ | {total_cases} |")
        report.append(f"| é€šè¿‡ | {total_passed} |")
        report.append(f"| å¤±è´¥ | {total_cases - total_passed} |")
        report.append(f"| é€šè¿‡ç‡ | {total_passed/max(1,total_cases)*100:.1f}% |")
        
        # ç»´åº¦èƒ½åŠ›é›·è¾¾å›¾æ•°æ®
        report.append(f"\n## ğŸ•¸ï¸ ç»´åº¦èƒ½åŠ›åˆ†æ\n")
        report.append(f"### èƒ½åŠ›é›·è¾¾å›¾æ•°æ®\n")
        report.append(f"```")
        report.append(f"ç»´åº¦èƒ½åŠ›å€¼ (0-100, è¶Šé«˜è¶Šå¼º):")
        for dim, score in radar_data['dimensions'].items():
            bar = 'â–ˆ' * int(score / 5) + 'â–‘' * (20 - int(score / 5))
            report.append(f"  {dim:12} [{bar}] {score:.1f}")
        report.append(f"```\n")
        
        # ç»´åº¦è¯¦ç»†ç»Ÿè®¡
        report.append(f"### ç»´åº¦è¯¦ç»†ç»Ÿè®¡\n")
        report.append(f"| ç»´åº¦ | æ€»æ•° | é€šè¿‡ | å¤±è´¥ | é€šè¿‡ç‡ | å¹³å‡å¾—åˆ† | å¼±ç‚¹è¯„åˆ† |")
        report.append(f"|------|------|------|------|--------|----------|----------|")
        for dim, stats in profile.by_dimension.items():
            if stats.total > 0:
                report.append(
                    f"| {dim} | {stats.total} | {stats.passed} | {stats.failed} | "
                    f"{stats.pass_rate*100:.1f}% | {stats.avg_score*100:.1f}% | "
                    f"{'ğŸ”´' if stats.weakness_score > 0.5 else 'ğŸŸ¡' if stats.weakness_score > 0.3 else 'ğŸŸ¢'} {stats.weakness_score:.2f} |"
                )
        
        # éš¾åº¦èƒ½åŠ›åˆ†æ
        report.append(f"\n## ğŸ“ˆ éš¾åº¦èƒ½åŠ›åˆ†æ\n")
        report.append(f"```")
        report.append(f"å„éš¾åº¦é€šè¿‡ç‡:")
        for diff in ['easy', 'medium', 'difficult']:
            stats = profile.by_difficulty.get(diff, DimensionStats())
            if stats.total > 0:
                bar = 'â–ˆ' * int(stats.pass_rate * 20) + 'â–‘' * (20 - int(stats.pass_rate * 20))
                report.append(f"  {diff:10} [{bar}] {stats.pass_rate*100:.1f}%")
        report.append(f"```\n")
        
        # è®¾å¤‡èƒ½åŠ›åˆ†æ
        report.append(f"\n## ğŸ  è®¾å¤‡èƒ½åŠ›åˆ†æ\n")
        report.append(f"| è®¾å¤‡ | æ€»æ•° | é€šè¿‡ç‡ | å¼±ç‚¹è¯„åˆ† |")
        report.append(f"|------|------|--------|----------|")
        sorted_devices = sorted(
            profile.by_device.items(), 
            key=lambda x: x[1].weakness_score, 
            reverse=True
        )
        for device, stats in sorted_devices:
            if stats.total > 0:
                icon = 'ğŸ”´' if stats.weakness_score > 0.5 else 'ğŸŸ¡' if stats.weakness_score > 0.3 else 'ğŸŸ¢'
                report.append(f"| {device} | {stats.total} | {stats.pass_rate*100:.1f}% | {icon} {stats.weakness_score:.2f} |")
        
        # èƒ½åŠ›è¾¹ç•Œ
        report.append(f"\n## ğŸš§ èƒ½åŠ›è¾¹ç•Œ\n")
        if profile.boundary_found:
            report.append(f"æ£€æµ‹åˆ°ä»¥ä¸‹èƒ½åŠ›è¾¹ç•Œï¼š\n")
            for dim, boundary_diff in profile.boundary_found.items():
                report.append(f"- **{dim}**: åœ¨ `{boundary_diff}` éš¾åº¦å¼€å§‹æ˜¾è‘—ä¸‹é™")
        else:
            report.append(f"æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„èƒ½åŠ›è¾¹ç•Œï¼ˆå¯èƒ½éœ€è¦æ›´å¤šæµ‹è¯•ï¼‰")
        
        # ä¸»è¦å¼±ç‚¹
        report.append(f"\n## âš ï¸ ä¸»è¦å¼±ç‚¹ (Top 5)\n")
        analyzer = WeaknessAnalyzer()
        analyzer.profile = profile
        top_weaknesses = analyzer.get_top_weaknesses(5)
        for i, (w_type, w_name, w_score) in enumerate(top_weaknesses, 1):
            severity = 'ğŸ”´ ä¸¥é‡' if w_score > 0.7 else 'ğŸŸ¡ ä¸­ç­‰' if w_score > 0.4 else 'ğŸŸ¢ è½»å¾®'
            report.append(f"{i}. **{w_type}: {w_name}** - å¼±ç‚¹åˆ†æ•°: {w_score:.2f} ({severity})")
        
        # æµ‹è¯•è½®æ¬¡å†å²
        report.append(f"\n## ğŸ“ æµ‹è¯•è½®æ¬¡å†å²\n")
        report.append(f"| è½®æ¬¡ | ç”¨ä¾‹æ•° | é€šè¿‡ | å¤±è´¥ | é€šè¿‡ç‡ | èšç„¦é¢†åŸŸ |")
        report.append(f"|------|--------|------|------|--------|----------|")
        for i, r in enumerate(round_history, 1):
            focus = r.get('focus', 'initial')
            report.append(
                f"| {i} | {r.get('total_cases', 0)} | {r.get('passed', 0)} | "
                f"{r.get('failed', 0)} | {r.get('pass_rate', 0)*100:.1f}% | {focus} |"
            )
        
        # å¤±è´¥ç”¨ä¾‹ç¤ºä¾‹
        report.append(f"\n## ğŸ“‹ å¤±è´¥ç”¨ä¾‹ç¤ºä¾‹ (æœ€å¤šæ˜¾ç¤º 5 ä¸ª)\n")
        for i, result in enumerate(profile.failed_cases[:5], 1):
            case = result.test_case
            report.append(f"### å¤±è´¥ç”¨ä¾‹ {i}: {case.get('scenario_id', 'unknown')}")
            report.append(f"- **éš¾åº¦**: {case.get('difficulty')}")
            report.append(f"- **ç»´åº¦**: {case.get('dimension')}")
            report.append(f"- **æè¿°**: {case.get('description')}")
            report.append(f"- **å¾—åˆ†**: {result.score}/{result.max_score}")
            if result.errors:
                report.append(f"- **é”™è¯¯**: {result.errors[:3]}")
            report.append("")
        
        # å»ºè®®
        report.append(f"\n## ğŸ’¡ æ”¹è¿›å»ºè®®\n")
        if top_weaknesses:
            report.append(f"åŸºäºå¼±ç‚¹åˆ†æï¼Œå»ºè®®é‡ç‚¹æ”¹è¿›ä»¥ä¸‹é¢†åŸŸï¼š\n")
            for w_type, w_name, w_score in top_weaknesses[:3]:
                if w_type == 'dimension':
                    report.append(f"1. **{w_name} ç»´åº¦**: åŠ å¼º {self._get_dimension_advice(w_name)}")
                else:
                    report.append(f"1. **{w_name} è®¾å¤‡**: åŠ å¼ºå¯¹è¯¥è®¾å¤‡çš„ç†è§£å’Œæ“ä½œèƒ½åŠ›")
        
        # å†™å…¥æ–‡ä»¶
        report_content = '\n'.join(report)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # åŒæ—¶ç”Ÿæˆ JSON æ•°æ®ï¼ˆä¾›ç¨‹åºè¯»å–ï¼‰
        json_file = self.output_dir / f"weakness_data_{timestamp}.json"
        json_data = {
            "agent_name": agent_name,
            "timestamp": timestamp,
            "radar_data": radar_data,
            "round_history": round_history,
            "top_weaknesses": [
                {"type": t, "name": n, "score": s} 
                for t, n, s in top_weaknesses
            ],
            "dimension_stats": {
                dim: {
                    "total": stats.total,
                    "passed": stats.passed,
                    "pass_rate": stats.pass_rate,
                    "avg_score": stats.avg_score,
                    "weakness_score": stats.weakness_score
                }
                for dim, stats in profile.by_dimension.items()
                if stats.total > 0
            },
            "difficulty_stats": {
                diff: {
                    "total": stats.total,
                    "passed": stats.passed,
                    "pass_rate": stats.pass_rate
                }
                for diff, stats in profile.by_difficulty.items()
                if stats.total > 0
            }
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"ğŸ“Š æ•°æ®å·²ç”Ÿæˆ: {json_file}")
        self.last_report_path = report_file
        self.last_data_path = json_file
        
        return str(report_file)
    
    def _compute_radar_data(self, profile: WeaknessProfile) -> dict:
        """è®¡ç®—é›·è¾¾å›¾æ•°æ®"""
        dimensions = {}
        for dim, stats in profile.by_dimension.items():
            if stats.total > 0:
                # èƒ½åŠ›åˆ†æ•° = 1 - å¼±ç‚¹åˆ†æ•°ï¼Œè½¬æ¢ä¸º 0-100
                dimensions[dim] = (1 - stats.weakness_score) * 100
            else:
                dimensions[dim] = 50  # æœªæµ‹è¯•çš„ç»™ä¸­é—´å€¼
        
        return {"dimensions": dimensions}
    
    def _get_dimension_advice(self, dimension: str) -> str:
        """è·å–ç»´åº¦æ”¹è¿›å»ºè®®"""
        advice = {
            "precision": "ç²¾ç¡®æŒ‡ä»¤ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›",
            "ambiguous": "æ¨¡ç³ŠæŒ‡ä»¤çš„æ¨ç†å’Œç†è§£èƒ½åŠ›",
            "conflict": "å†²çªæŒ‡ä»¤çš„æ£€æµ‹å’Œå¤„ç†èƒ½åŠ›",
            "memory": "ä¸Šä¸‹æ–‡è®°å¿†å’ŒçŠ¶æ€è¿½è¸ªèƒ½åŠ›",
            "noise": "å™ªå£°è¿‡æ»¤å’Œå…³é”®ä¿¡æ¯æå–èƒ½åŠ›"
        }
        return advice.get(dimension, "ç›¸å…³èƒ½åŠ›")


# ============== ä¸»å¾ªç¯ ==============

class AdaptiveTestLoop:
    """è‡ªé€‚åº”å¯¹æŠ—æµ‹è¯•ä¸»å¾ªç¯"""
    
    def __init__(
        self,
        green_agent: GreenAgent,
        baseline_agent: BaselineAgent,
        output_dir: Path = Path("test_results")
    ):
        self.green_agent = green_agent
        self.baseline_agent = baseline_agent
        self.evaluator = AdaptiveEvaluator(baseline_agent)
        self.analyzer = WeaknessAnalyzer()
        self.generator = AdaptiveGenerator(green_agent)
        self.reporter = ReportGenerator(output_dir)
        
        self.round_history = []
        self.all_results: List[TestResult] = []
    
    def run(
        self,
        max_rounds: int = 5,
        initial_per_dim: int = 10,
        targeted_per_weakness: int = 5,
        convergence_threshold: float = 0.05,
        agent_name: str = "Purple Agent"
    ) -> str:
        """
        è¿è¡Œè‡ªé€‚åº”æµ‹è¯•å¾ªç¯
        
        Args:
            max_rounds: æœ€å¤§æµ‹è¯•è½®æ•°
            initial_per_dim: åˆå§‹æ¯ä¸ªç»´åº¦ç”Ÿæˆçš„ç”¨ä¾‹æ•°
            targeted_per_weakness: æ¯ä¸ªå¼±ç‚¹é’ˆå¯¹æ€§ç”Ÿæˆçš„ç”¨ä¾‹æ•°
            convergence_threshold: æ”¶æ•›é˜ˆå€¼ï¼ˆè¿ç»­ä¸¤è½®é€šè¿‡ç‡å˜åŒ–å°äºæ­¤å€¼æ—¶åœæ­¢ï¼‰
            agent_name: Agent åç§°ï¼ˆç”¨äºæŠ¥å‘Šï¼‰
        
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        
        print("=" * 70)
        print("ğŸ”„ Adaptive Adversarial Testing Loop")
        print("=" * 70)
        print(f"ç›®æ ‡ Agent: {agent_name}")
        print(f"æœ€å¤§è½®æ•°: {max_rounds}")
        print(f"åˆå§‹ç”¨ä¾‹/ç»´åº¦: {initial_per_dim}")
        print(f"æ”¶æ•›é˜ˆå€¼: {convergence_threshold}")
        print("=" * 70)
        
        last_pass_rate = None
        
        for round_num in range(1, max_rounds + 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“ ç¬¬ {round_num} è½®æµ‹è¯•")
            print(f"{'='*70}")
            
            # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            if round_num == 1:
                # ç¬¬ä¸€è½®ï¼šå‡åŒ€ç”Ÿæˆå„ç»´åº¦ç”¨ä¾‹
                test_cases = self._generate_initial_cases(initial_per_dim)
                focus = "initial_balanced"
            else:
                # åç»­è½®ï¼šé’ˆå¯¹å¼±ç‚¹ç”Ÿæˆ
                top_weaknesses = self.analyzer.get_top_weaknesses(3)
                test_cases = self.generator.generate_targeted(
                    top_weaknesses, 
                    targeted_per_weakness,
                    difficulty_boost=True
                )
                focus = f"targeted_{top_weaknesses[0][1] if top_weaknesses else 'unknown'}"
            
            if not test_cases:
                print("âš ï¸ æœªèƒ½ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç»“æŸå¾ªç¯")
                break
            
            # è¯„ä¼°
            print(f"\nğŸ“ è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
            results = self.evaluator.evaluate_batch(
                [tc.model_dump() if hasattr(tc, 'model_dump') else tc for tc in test_cases]
            )
            
            # ç»Ÿè®¡æœ¬è½®ç»“æœ
            passed = sum(1 for r in results if r.passed)
            failed = len(results) - passed
            pass_rate = passed / max(1, len(results))
            
            round_info = {
                "round": round_num,
                "total_cases": len(results),
                "passed": passed,
                "failed": failed,
                "pass_rate": pass_rate,
                "focus": focus
            }
            self.round_history.append(round_info)
            self.all_results.extend(results)
            
            print(f"\nğŸ“Š æœ¬è½®ç»“æœ: {passed}/{len(results)} é€šè¿‡ ({pass_rate*100:.1f}%)")
            
            # æ›´æ–°å¼±ç‚¹åˆ†æ
            self.analyzer.analyze(results)
            
            # æ˜¾ç¤ºå½“å‰å¼±ç‚¹
            top_weaknesses = self.analyzer.get_top_weaknesses(3)
            if top_weaknesses:
                print(f"\nâš ï¸ å½“å‰ä¸»è¦å¼±ç‚¹:")
                for w_type, w_name, w_score in top_weaknesses:
                    print(f"   - {w_type}: {w_name} (å¼±ç‚¹åˆ†æ•°: {w_score:.2f})")
            
            # æ£€æŸ¥æ”¶æ•›
            if last_pass_rate is not None:
                rate_change = abs(pass_rate - last_pass_rate)
                if rate_change < convergence_threshold:
                    print(f"\nâœ… é€šè¿‡ç‡å˜åŒ– ({rate_change:.3f}) å°äºé˜ˆå€¼ ({convergence_threshold})ï¼Œèƒ½åŠ›è¾¹ç•Œå·²ç¨³å®š")
                    break
            
            last_pass_rate = pass_rate
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»´åº¦éƒ½å·²æ¢æµ‹åˆ°è¾¹ç•Œ
            if len(self.analyzer.profile.boundary_found) >= len(DIMENSIONS):
                print(f"\nâœ… æ‰€æœ‰ç»´åº¦èƒ½åŠ›è¾¹ç•Œå·²æ¢æµ‹å®Œæˆ")
                break
        
        # ç”ŸæˆæŠ¥å‘Š
        print(f"\n{'='*70}")
        print("ğŸ“„ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        print(f"{'='*70}")
        
        report_path = self.reporter.generate_report(
            self.analyzer.profile,
            self.round_history,
            agent_name
        )
        
        return report_path
    
    def _generate_initial_cases(self, per_dim: int) -> List[TestCase]:
        """ç”Ÿæˆåˆå§‹æµ‹è¯•ç”¨ä¾‹ï¼ˆå„ç»´åº¦å‡åŒ€åˆ†å¸ƒï¼Œä» easy å¼€å§‹ï¼‰"""
        cases = []
        
        for dim in DIMENSIONS:
            # easy å°‘ä¸€äº›ï¼Œmedium/difficult å¤šä¸€äº›
            easy_count = per_dim // 3
            medium_count = per_dim // 3
            difficult_count = per_dim - easy_count - medium_count
            
            print(f"\nğŸŸ¢ ç”Ÿæˆ {dim} ç»´åº¦ç”¨ä¾‹ (easy:{easy_count}, medium:{medium_count}, difficult:{difficult_count})")
            
            for diff, count in [('easy', easy_count), ('medium', medium_count), ('difficult', difficult_count)]:
                for i in range(count):
                    case = self.green_agent.generate_single_case(
                        difficulty=diff,
                        dimension=dim,
                        scenario_number=len(cases) + 1
                    )
                    if case:
                        cases.append(case)
        
        return cases


# ============== å‘½ä»¤è¡Œæ¥å£ ==============

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Adaptive Adversarial Testing Loop - è‡ªé€‚åº”å¯¹æŠ—æµ‹è¯•å¾ªç¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæµ‹è¯•æ¡†æ¶ï¼‰
  python adaptive_loop.py --rounds 5 --initial-per-dim 10
  
  # ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ä½œä¸ºè¢«æµ‹ Agent
  python adaptive_loop.py --agent-type ollama --agent-model qwen2.5:7b
  
  # ä½¿ç”¨ OpenAI API ä½œä¸ºè¢«æµ‹ Agent
  python adaptive_loop.py --agent-type openai --agent-model gpt-4o
  
  # ä½¿ç”¨ Anthropic Claude ä½œä¸ºè¢«æµ‹ Agent
  python adaptive_loop.py --agent-type anthropic --agent-model claude-3-5-sonnet-20241022
  
  # ä½¿ç”¨ A2A åè®®è¿æ¥è¿œç¨‹ Agent
  python adaptive_loop.py --agent-type a2a --agent-url https://agent.example.com
  
  # ä½¿ç”¨ MCP åè®®è¿æ¥ Agent
  python adaptive_loop.py --agent-type mcp --agent-url http://localhost:3000
  
  # ä½¿ç”¨é€šç”¨ HTTP API Agent
  python adaptive_loop.py --agent-type http --agent-url https://api.example.com/chat
  
  # å¸¦é”™è¯¯ç‡çš„æ¨¡æ‹Ÿï¼ˆæµ‹è¯•æ¡†æ¶é²æ£’æ€§ï¼‰
  python adaptive_loop.py --agent-type imperfect --error-rate 0.3
  
  # æŒ‡å®š Green Agent ä½¿ç”¨è¿œç¨‹ API
  python adaptive_loop.py --green-provider openai --green-model gpt-4o-mini
        """
    )
    
    # æµ‹è¯•å¾ªç¯å‚æ•°
    parser.add_argument("--rounds", "-r", type=int, default=5, help="æœ€å¤§æµ‹è¯•è½®æ•°")
    parser.add_argument("--initial-per-dim", "-i", type=int, default=10, help="åˆå§‹æ¯ç»´åº¦ç”¨ä¾‹æ•°")
    parser.add_argument("--targeted-per-weakness", "-t", type=int, default=5, help="æ¯å¼±ç‚¹é’ˆå¯¹æ€§ç”¨ä¾‹æ•°")
    parser.add_argument("--convergence", "-c", type=float, default=0.05, help="æ”¶æ•›é˜ˆå€¼")
    parser.add_argument("--agent-name", "-n", default="", help="Agent åç§°ï¼ˆç”¨äºæŠ¥å‘Šï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰")
    parser.add_argument("--output-dir", "-o", default="test_results", help="è¾“å‡ºç›®å½•")
    
    # Green Agent å‚æ•°
    parser.add_argument("--green-provider", default="ollama", 
                        choices=["ollama", "openai", "anthropic", "deepseek", "openrouter"],
                        help="Green Agent API æä¾›è€…")
    parser.add_argument("--green-model", default="qwen2.5-coder:7b", help="Green Agent æ¨¡å‹")
    parser.add_argument("--green-base-url", default=None, help="Green Agent API åŸºç¡€ URL")
    parser.add_argument("--green-api-key", default=None, help="Green Agent API å¯†é’¥")
    
    # Baseline Agent å‚æ•°
    parser.add_argument("--agent-type", 
                        choices=["mock", "simulated", "imperfect", "ollama", "openai", "anthropic", 
                                 "purple", "a2a", "mcp", "http"],
                        default="mock", help="è¢«æµ‹ Agent ç±»å‹")
    parser.add_argument("--agent-model", default="qwen2.5:7b", help="è¢«æµ‹ Agent æ¨¡å‹ï¼ˆç”¨äº ollama/openai/anthropicï¼‰")
    parser.add_argument("--agent-base-url", default=None, help="è¢«æµ‹ Agent API åŸºç¡€ URL")
    parser.add_argument("--agent-url", default=None, help="è¢«æµ‹ Agent URLï¼ˆç”¨äº a2a/mcp/httpï¼‰")
    parser.add_argument("--agent-api-key", default=None, help="è¢«æµ‹ Agent API å¯†é’¥")
    parser.add_argument("--error-rate", type=float, default=0.2, help="imperfect æ¨¡å¼çš„é”™è¯¯ç‡")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ– Green Agent
    print("ğŸŸ¢ åˆå§‹åŒ– Green Agent...")
    green_agent = GreenAgent(
        model=args.green_model,
        provider=args.green_provider,
        base_url=args.green_base_url,
        api_key=args.green_api_key,
        max_retries=3
    )
    print(f"   Provider: {args.green_provider}, Model: {args.green_model}")
    
    # åˆå§‹åŒ–ç¯å¢ƒ
    env = SmartHomeEnv()
    
    # åˆå§‹åŒ– Baseline Agent
    print("ğŸŸ£ åˆå§‹åŒ–è¢«æµ‹ Agent...")
    
    # ç¡®å®š Agent åç§°
    agent_name = args.agent_name
    if not agent_name:
        if args.agent_type in ["mock", "simulated"]:
            agent_name = "Simulated Perfect Agent"
        elif args.agent_type == "imperfect":
            agent_name = f"Simulated Agent (error_rate={args.error_rate})"
        elif args.agent_type in ["a2a", "mcp", "http"]:
            agent_name = f"{args.agent_type.upper()} Agent ({args.agent_url})"
        else:
            agent_name = f"{args.agent_type.title()} Agent ({args.agent_model})"
    
    # æ„å»º Agent å‚æ•°
    agent_kwargs = {}
    if args.agent_type in ["ollama", "openai", "anthropic"]:
        agent_kwargs['model'] = args.agent_model
        if args.agent_base_url:
            agent_kwargs['base_url'] = args.agent_base_url
        if args.agent_api_key:
            agent_kwargs['api_key'] = args.agent_api_key
    elif args.agent_type == "a2a":
        if not args.agent_url:
            print("âŒ é”™è¯¯: --agent-url å‚æ•°æ˜¯ A2A ç±»å‹å¿…éœ€çš„")
            return
        agent_kwargs['agent_url'] = args.agent_url
        if args.agent_api_key:
            agent_kwargs['api_key'] = args.agent_api_key
    elif args.agent_type == "mcp":
        if not args.agent_url:
            print("âŒ é”™è¯¯: --agent-url å‚æ•°æ˜¯ MCP ç±»å‹å¿…éœ€çš„")
            return
        agent_kwargs['server_url'] = args.agent_url
    elif args.agent_type == "http":
        if not args.agent_url:
            print("âŒ é”™è¯¯: --agent-url å‚æ•°æ˜¯ HTTP ç±»å‹å¿…éœ€çš„")
            return
        agent_kwargs['url'] = args.agent_url
        if args.agent_api_key:
            agent_kwargs['api_key'] = args.agent_api_key
    elif args.agent_type == "imperfect":
        agent_kwargs['error_rate'] = args.error_rate
    
    baseline_agent = BaselineAgent(env, agent_type=args.agent_type, **agent_kwargs)
    print(f"   Type: {args.agent_type}")
    print(f"   Name: {agent_name}")
    
    # è¿è¡Œå¾ªç¯
    loop = AdaptiveTestLoop(
        green_agent=green_agent,
        baseline_agent=baseline_agent,
        output_dir=Path(args.output_dir)
    )
    
    report_path = loop.run(
        max_rounds=args.rounds,
        initial_per_dim=args.initial_per_dim,
        targeted_per_weakness=args.targeted_per_weakness,
        convergence_threshold=args.convergence,
        agent_name=agent_name
    )
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼æŠ¥å‘Š: {report_path}")


if __name__ == "__main__":
    main()
